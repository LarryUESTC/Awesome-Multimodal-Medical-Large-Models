# Awesome-Multimodal-Medical-Large-Models
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://makeapullrequest.com)

ğŸ”¥  This is a collection of awesome (not comprehensive but high quality) papers about *Multimodal Medical Data Analysis*

âœ… Continue update 2024.1.15

ğŸ˜Š If you have any recommended papers, pls feel free to contact me ([Email](larrypengliang@gmail.com) or WeChat: pl15828102252)

![Alt Text](fig/Amen.gif)

# Papers


1. ğŸ“œ **Towards Generalist Biomedical AI (Med-PaLM M)** ğŸ”¥ 
    - ğŸ—“ï¸ Date: 2023.07
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, et.al.,
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2307.14334.pdf)
    - ğŸ’» [GitHub](https://github.com/kyegomez/Med-PaLM)
    - ğŸ“Œ Keys: Foundation model; Multi-modalities; Multi-tasks
    - ğŸ”¬ Datasets: MultiMedBench

2. ğŸ“œ **Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data** ğŸ”¥ 
    - ğŸ—“ï¸ Date: 2023.07
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2308.02463.pdf)
    - ğŸ’» [GitHub](https://github.com/chaoyi-wu/RadFM)
    - ğŸ“Œ Keys: Foundation model, multi-modalities, multi-tasks
    - ğŸ”¬ Datasets: MedMD; RadMD
    
3. ğŸ“œ **CLIP in medical imaging: A comprehensive survey**
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2312.07353.pdf)
    - ğŸ’» [GitHub](https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging)
    - ğŸ“Œ Keys: CLIP; Survey

4. ğŸ“œ **CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare**
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– AAAI 2024
    - ğŸ§‘â€ Authors: Akash Ghosh, Arkadeep Acharya, Raghav Jain, Sriparna Saha, Aman Chadha, Setu Sinha
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2312.11541.pdf)
    - ğŸ’» [GitHub](https://github.com/AkashGhosh/CLIPSyntel-AAAI2024)
    - ğŸ“Œ Keys: CLIP; LLM; Question-summarization
    - ğŸ”¬ Datasets: MMQS

5. ğŸ“œ **Multimodal biomedical AI**
    - ğŸ—“ï¸ Date: 2022.09
    - ğŸ“– Nature Medicine 2022
    - ğŸ§‘â€ Authors: JuliÃ¡n N. Acosta, Guido J. Falcone, Pranav Rajpurkar, Eric J. Topol 
    - ğŸ“„ [PDF](https://www.nature.com/articles/s41591-022-01981-2)
    - ğŸ“Œ Keys: Multimodal; Survey

6. ğŸ“œ **A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges**
    - ğŸ—“ï¸ Date: 2023.11
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2311.05112.pdf)
    - ğŸ’» [GitHub](https://github.com/AI-in-Health/MedLLMsPracticalGuide)
    - ğŸ“Œ Keys: LLM; Survey

7. ğŸ“œ **Large-scale Long-tailed Disease Diagnosis on Radiology Images**ğŸ”¥
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Qiaoyu Zheng, Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2312.16151)
    - ğŸ’» [GitHub](https://github.com/qiaoyu-zheng/RP3D-Diag)
    - ğŸ“Œ Keys: Foundation model; Multi-modalities; Multi-tasks
    - ğŸ”¬ Datasets:  ICD-10-CM

8. ğŸ“œ **On the Challenges and Perspectives of Foundation Models for Medical Image Analysis**
    - ğŸ—“ï¸ Date: 2023.06
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Shaoting Zhang, Dimitris Metaxas
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2306.05705.pdf)
    - ğŸ“Œ Keys: Foundation model; Challenges and Perspectives; Survey

9. ğŸ“œ **One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts**ğŸ”¥
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
    - ğŸ“„ [PDF](https://arxiv.org/abs/2312.17183)
    - ğŸ’» [GitHub](https://zhaoziheng.github.io/MedUniSeg/)
    - ğŸ“Œ Keys: Foundation model; Segmentation;  Multimodal Knowledge

10. ğŸ“œ **Med-Flamingo: a Multimodal Medical Few-shot Learner**
    - ğŸ—“ï¸ Date: 2023.07
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav Rajpurkar, Jure Leskovec
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2307.15189.pdf)
    - ğŸ’» [GitHub](https://github.com/snap-stanford/med-flamingo)
    - ğŸ“Œ Keys: VQA; Few-shot learning; Fine-turning

11. ğŸ“œ **MedCLIP: Contrastive Learning from Unpaired Medical Images and Text**
    - ğŸ—“ï¸ Date: 2022.10
    - ğŸ“– EMNLP 2022
    - ğŸ§‘â€ Authors:Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2307.15189.pdf)
    - ğŸ’» [GitHub](https://github.com/RyanWangZf/MedCLIP)
    - ğŸ“Œ Keys: CLIP; Zero-shot

12. ğŸ“œ **PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents**
    - ğŸ—“ï¸ Date: 2023.03
    - ğŸ“– MICCAI 2023
    - ğŸ§‘â€ Authors:Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2307.15189.pdf)
    - ğŸ’» [GitHub](https://github.com/WeixiongLin/PMC-CLIP)
    - ğŸ“Œ Keys: CLIP;  Image-caption construction

13. ğŸ“œ **Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision**
    - ğŸ—“ï¸ Date: 2023.10
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors:Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, Amirhossein Kazerouni, Islem Rekik, Dorit Merhof
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2310.18689.pdf)
    - ğŸ’» [GitHub](https://github.com/xmindflow/Awesome-Foundation-Models-in-Medical-Imaging)
    - ğŸ“Œ Keys: Foundational models;  Survey

14. ğŸ“œ **MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts**
    - ğŸ—“ï¸ Date: 2023.06
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Bang Yang, Asif Raza, Yuexian Zou, Tong Zhang
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2306.05642.pdf)
    - ğŸ’» [GitHub](https://github.com/Qybc/MedBLIP)
    - ğŸ“Œ Keys: BLIP2; VQA

15. ğŸ“œ **Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning**
    - ğŸ—“ï¸ Date: 2023.11
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Zhenyu Zhang, Benlu Wang, Weijie Liang, Yizhi Li, Xuechen Guo, Guanhong Wang, Shiyan Li, Gaoang Wang
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2311.01004.pdf)
    - ğŸ“Œ Keys: BLIP2; SAM

16. ğŸ“œ **Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost**
    - ğŸ—“ï¸ Date: 2023.06
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Juexiao Zhou and Xiuying Chen and Xin Gao
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2306.10765.pdf)
    - ğŸ’» [GitHub](https://github.com/JoshuaChou2018/MedAGI)
    - ğŸ“Œ Keys: MedChat

# Base papers

1. ğŸ“œ **CLIP**
    - ğŸ—“ï¸ Date: 2021.3
    - ğŸ“– ICML 2021
    - ğŸ§‘â€ Authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2103.00020.pdf)
    - ğŸ’» [GitHub](https://github.com/openai/CLIP)
    - ğŸ“Œ Keys: Contrastive Language-Image Pre-Training

2. ğŸ“œ **MaPLe: Multi-modal Prompt Learning**
    - ğŸ—“ï¸ Date: 2023
    - ğŸ“– CVPR 2023
    - ğŸ§‘â€ Authors: Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2210.03117.pdf)
    - ğŸ’» [GitHub](https://github.com/muzairkhattak/multimodal-prompt-learning)
    - ğŸ“Œ Keys: Multi-modal Prompt, based on ClIP

3. ğŸ“œ **Meta-Transformer: A Unified Framework for Multimodal Learning**
    - ğŸ—“ï¸ Date: 2023
    - ğŸ“– CVPR 2023
    - ğŸ§‘â€ Authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2307.10802.pdf)
    - ğŸ’» [GitHub](https://github.com/invictus717/MetaTransformer)
    - ğŸ“Œ Keys: Unified multi-modal Modal, token is all you need

4. ğŸ“œ **ImageBind: One Embedding Space To Bind Them All**
    - ğŸ—“ï¸ Date: 2023
    - ğŸ“– CVPR 2023
    - ğŸ§‘â€ Authors: Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2305.05665.pdf)
    - ğŸ’» [GitHub](https://facebookresearch.github.io/ImageBind)
    - ğŸ“Œ Keys: Unified multi-modal Modal, token is all you need

5. ğŸ“œ **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**
    - ğŸ—“ï¸ Date: 2023.01
    - ğŸ“– ICML 2023
    - ğŸ§‘â€ Authors: Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2301.12597.pdf)
    - ğŸ’» [GitHub](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
    - ğŸ“Œ Keys: Multi-modal; Q-Former
# Related papers

## SAM

## LLM

1. ğŸ“œ **Pixel Aligned Language Models**
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2312.09237.pdf)
    - ğŸ’» [GitHub](https://jerryxu.net/PixelLLM/)
    - ğŸ“Œ Keys: LLM

2. ğŸ“œ **PixelLM: Pixel Reasoning with Large Multimodal Model**
    - ğŸ—“ï¸ Date: 2023.12
    - ğŸ“– arXiv
    - ğŸ§‘â€ Authors: Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, Xiaojie Jin
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2312.09237.pdf)
    - ğŸ’» [GitHub](https://pixellm.github.io/)
    - ğŸ“Œ Keys: LLM

3. ğŸ“œ **SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs**
    - ğŸ—“ï¸ Date: 2023.06
    - ğŸ“– NeurIPS 2023
    - ğŸ§‘â€ Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
    - ğŸ“„ [PDF](https://arxiv.org/pdf/2306.17842.pdf)
    - ğŸ“Œ Keys: LLM; Multimodal


# Multimodal medical datasets

| Name             | Modality                              | Size  | Task | Organ | Link |
|------------------|---------------------------------------|-------|------|-------|------|
| ADNI             | MRI, PET, fMRI, clinical, Gene, Table |       |      |       |      |
| ABIDE            | MRI, PET                              |       |      |       |      |
| INSPECT          |                                       |       |      |       |      |
| MIMIC-CXR        | X-ray-text pairs                      |       |      |       |      |
| UK Biobank       |                                       |       |      |       |      |
| MultiMedBench    |                                       |       |      |       |      |
| MedMD            |                                       |       |      |       |      |
| RadMD            |                                       |       |      |       |      |
| ICD-10-CM        |                                       |       |      |       |      |
| TCGA(-LUAD/STAD) | WSIs, clinical, Gene                  |       |      |       |      |
| MedICaT          | CT-text pairs                         |       |      |       |      |
| ROCO             | CT-text pairs                         | 217K  |      |       |      |
